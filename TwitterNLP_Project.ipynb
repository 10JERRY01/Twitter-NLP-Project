{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "2JdGCIsMV5zS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def load_conll_data(file_path):\n",
        "    \"\"\"\n",
        "    Loads data from a CoNLL formatted file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the CoNLL file.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two lists:\n",
        "               - sentences (list of lists of words)\n",
        "               - tags (list of lists of tags)\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    tags = []\n",
        "    current_sentence = []\n",
        "    current_tags = []\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "        return [], []\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line == \"\": # End of a sentence\n",
        "                    if current_sentence:\n",
        "                        sentences.append(current_sentence)\n",
        "                        tags.append(current_tags)\n",
        "                        current_sentence = []\n",
        "                        current_tags = []\n",
        "                else:\n",
        "                    parts = line.split() # Default split handles potential tabs/spaces\n",
        "                    if len(parts) >= 2:\n",
        "                        word = parts[0]\n",
        "                        tag = parts[-1] # Assume tag is the last element\n",
        "                        current_sentence.append(word)\n",
        "                        current_tags.append(tag)\n",
        "                    else:\n",
        "                        # Handle potential malformed lines, e.g., lines with only a word or tag\n",
        "                        print(f\"Skipping malformed line: '{line}' in file {file_path}\")\n",
        "\n",
        "            # Add the last sentence if the file doesn't end with a blank line\n",
        "            if current_sentence:\n",
        "                sentences.append(current_sentence)\n",
        "                tags.append(current_tags)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file {file_path}: {e}\")\n",
        "        return [], []\n",
        "\n",
        "    return sentences, tags\n",
        "\n",
        "def get_data_stats(sentences, tags, dataset_name=\"Dataset\"):\n",
        "    \"\"\"Calculates and prints basic statistics about the loaded data.\"\"\"\n",
        "    num_sentences = len(sentences)\n",
        "    num_tokens = sum(len(s) for s in sentences)\n",
        "    all_tags = [tag for tag_list in tags for tag in tag_list]\n",
        "    unique_tags = sorted(list(set(all_tags)))\n",
        "    num_unique_tags = len(unique_tags)\n",
        "\n",
        "    print(f\"--- {dataset_name} Statistics ---\")\n",
        "    print(f\"Number of sentences: {num_sentences}\")\n",
        "    print(f\"Number of tokens: {num_tokens}\")\n",
        "    print(f\"Number of unique tags: {num_unique_tags}\")\n",
        "    print(f\"Unique tags: {unique_tags}\")\n",
        "    print(\"-\" * (len(dataset_name) + 18))\n",
        "    return unique_tags\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_file = \"wnut 16.txt.conll\"\n",
        "    test_file = \"wnut 16test.txt.conll\"\n",
        "\n",
        "    print(f\"Loading training data from: {train_file}\")\n",
        "    train_sentences, train_tags = load_conll_data(train_file)\n",
        "\n",
        "    print(f\"\\nLoading test data from: {test_file}\")\n",
        "    test_sentences, test_tags = load_conll_data(test_file)\n",
        "\n",
        "    if train_sentences and test_sentences:\n",
        "        print(\"\\nCalculating statistics...\")\n",
        "        train_unique_tags = get_data_stats(train_sentences, train_tags, \"Training Set\")\n",
        "        test_unique_tags = get_data_stats(test_sentences, test_tags, \"Test Set\")\n",
        "\n",
        "        # Check if tag sets are consistent (optional but good practice)\n",
        "        if set(train_unique_tags) == set(test_unique_tags):\n",
        "            print(\"\\nTag sets are consistent between training and test data.\")\n",
        "        else:\n",
        "            print(\"\\nWarning: Tag sets differ between training and test data.\")\n",
        "            print(f\"Tags only in train: {set(train_unique_tags) - set(test_unique_tags)}\")\n",
        "            print(f\"Tags only in test: {set(test_unique_tags) - set(train_unique_tags)}\")\n",
        "\n",
        "        print(\"\\nSample Data (First sentence):\")\n",
        "        if train_sentences:\n",
        "            print(\"Train Sentence:\", train_sentences[0])\n",
        "            print(\"Train Tags:\", train_tags[0])\n",
        "        if test_sentences:\n",
        "            print(\"Test Sentence:\", test_sentences[0])\n",
        "            print(\"Test Tags:\", test_tags[0])\n",
        "\n",
        "    else:\n",
        "        print(\"\\nCould not load data properly. Please check file paths and format.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmmLbVfQMGnq",
        "outputId": "21e3285c-364c-43fa-8862-57aad74095b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training data from: wnut 16.txt.conll\n",
            "\n",
            "Loading test data from: wnut 16test.txt.conll\n",
            "\n",
            "Calculating statistics...\n",
            "--- Training Set Statistics ---\n",
            "Number of sentences: 2394\n",
            "Number of tokens: 46469\n",
            "Number of unique tags: 21\n",
            "Unique tags: ['B-company', 'B-facility', 'B-geo-loc', 'B-movie', 'B-musicartist', 'B-other', 'B-person', 'B-product', 'B-sportsteam', 'B-tvshow', 'I-company', 'I-facility', 'I-geo-loc', 'I-movie', 'I-musicartist', 'I-other', 'I-person', 'I-product', 'I-sportsteam', 'I-tvshow', 'O']\n",
            "------------------------------\n",
            "--- Test Set Statistics ---\n",
            "Number of sentences: 3850\n",
            "Number of tokens: 61908\n",
            "Number of unique tags: 21\n",
            "Unique tags: ['B-company', 'B-facility', 'B-geo-loc', 'B-movie', 'B-musicartist', 'B-other', 'B-person', 'B-product', 'B-sportsteam', 'B-tvshow', 'I-company', 'I-facility', 'I-geo-loc', 'I-movie', 'I-musicartist', 'I-other', 'I-person', 'I-product', 'I-sportsteam', 'I-tvshow', 'O']\n",
            "--------------------------\n",
            "\n",
            "Tag sets are consistent between training and test data.\n",
            "\n",
            "Sample Data (First sentence):\n",
            "Train Sentence: ['@SammieLynnsMom', '@tg10781', 'they', 'will', 'be', 'all', 'done', 'by', 'Sunday', 'trust', 'me', '*wink*']\n",
            "Train Tags: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Test Sentence: ['New', 'Orleans', 'Mother', \"'s\", 'Day', 'Parade', 'shooting', '.', 'One', 'of', 'the', 'people', 'hurt', 'was', 'a', '10-year-old', 'girl', '.', 'WHAT', 'THE', 'HELL', 'IS', 'WRONG', 'WITH', 'PEOPLE', '?']\n",
            "Test Tags: ['B-other', 'I-other', 'I-other', 'I-other', 'I-other', 'I-other', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train BERT NER"
      ],
      "metadata": {
        "id": "AWMwSODuV-Uv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from transformers import BertTokenizerFast\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "# --- Data Loading Function (copied from preprocess_data.py) ---\n",
        "def load_conll_data(file_path):\n",
        "    \"\"\"Loads data from a CoNLL formatted file.\"\"\"\n",
        "    sentences = []\n",
        "    tags = []\n",
        "    current_sentence = []\n",
        "    current_tags = []\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "        return [], []\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line == \"\":\n",
        "                    if current_sentence:\n",
        "                        sentences.append(current_sentence)\n",
        "                        tags.append(current_tags)\n",
        "                        current_sentence = []\n",
        "                        current_tags = []\n",
        "                else:\n",
        "                    parts = line.split()\n",
        "                    if len(parts) >= 2:\n",
        "                        word = parts[0]\n",
        "                        tag = parts[-1]\n",
        "                        current_sentence.append(word)\n",
        "                        current_tags.append(tag)\n",
        "                    else:\n",
        "                        print(f\"Skipping malformed line: '{line}' in file {file_path}\")\n",
        "            if current_sentence:\n",
        "                sentences.append(current_sentence)\n",
        "                tags.append(current_tags)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file {file_path}: {e}\")\n",
        "        return [], []\n",
        "    return sentences, tags\n",
        "# --- End of Data Loading Function ---\n",
        "\n",
        "# --- Configuration ---\n",
        "TRAIN_FILE = \"wnut 16.txt.conll\"\n",
        "TEST_FILE = \"wnut 16test.txt.conll\"\n",
        "MODEL_NAME = 'bert-base-uncased'\n",
        "MAX_LEN = 128 # Max sequence length for BERT\n",
        "BATCH_SIZE = 16 # Adjust based on GPU memory\n",
        "TAG_MAP_PATH = \"tag_map_bert.pkl\" # Separate tag map for BERT potentially\n",
        "PREPARED_BERT_DATA_PATH = \"prepared_bert_data.pkl\"\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "print(\"Loading data...\")\n",
        "train_sentences, train_tags = load_conll_data(TRAIN_FILE)\n",
        "test_sentences, test_tags = load_conll_data(TEST_FILE) # Using test set for now, will split train later\n",
        "\n",
        "if not train_sentences or not test_sentences:\n",
        "    print(\"Failed to load data. Exiting.\")\n",
        "    exit()\n",
        "\n",
        "print(f\"Loaded {len(train_sentences)} training sentences and {len(test_sentences)} test sentences.\")\n",
        "\n",
        "# --- 2. Create Tag Mapping ---\n",
        "print(\"\\nCreating tag mapping...\")\n",
        "# Use tags from training data only to define the mapping\n",
        "all_tags_flat_train = [tag for sublist in train_tags for tag in sublist]\n",
        "unique_tags = sorted(list(set(all_tags_flat_train)))\n",
        "tag2idx = {tag: i for i, tag in enumerate(unique_tags)}\n",
        "idx2tag = {i: tag for tag, i in tag2idx.items()}\n",
        "n_tags = len(unique_tags)\n",
        "print(f\"Number of unique tags (from train set): {n_tags}\")\n",
        "print(f\"Tag mapping: {tag2idx}\")\n",
        "\n",
        "# Save tag mapping\n",
        "with open(TAG_MAP_PATH, 'wb') as f:\n",
        "    pickle.dump({'tag2idx': tag2idx, 'idx2tag': idx2tag, 'n_tags': n_tags}, f)\n",
        "print(f\"Tag mapping saved to {TAG_MAP_PATH}\")\n",
        "\n",
        "# --- 3. Tokenization and Label Alignment ---\n",
        "print(f\"\\nLoading tokenizer: {MODEL_NAME}...\")\n",
        "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def tokenize_and_align_labels(sentences, tags, tokenizer, tag2idx_map):\n",
        "    \"\"\"\n",
        "    Tokenizes sentences using the provided tokenizer and aligns the\n",
        "    corresponding NER tags to the generated tokens (WordPieces/subwords).\n",
        "    Uses -100 for special tokens and subsequent subword tokens,\n",
        "    so they are ignored by the loss function during training.\n",
        "    \"\"\"\n",
        "    # Ensure 'O' tag exists for default assignment\n",
        "    o_tag_idx = tag2idx_map.get('O')\n",
        "    if o_tag_idx is None:\n",
        "        # This should ideally not happen if 'O' is in the training data,\n",
        "        # but handle it just in case. Assign a default index (e.g., 0)\n",
        "        # or raise an error. Here, we'll print a warning and use 0.\n",
        "        print(\"Warning: 'O' tag not found in tag2idx mapping. Using index 0 as default.\")\n",
        "        o_tag_idx = 0 # Or choose another appropriate default/error handling\n",
        "\n",
        "    tokenized_inputs = tokenizer(sentences, truncation=True, is_split_into_words=True, padding='max_length', max_length=MAX_LEN)\n",
        "    labels = []\n",
        "    for i, label_list in enumerate(tags):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None: # Special tokens like [CLS], [SEP]\n",
        "                label_ids.append(-100) # Ignore special tokens in loss calculation\n",
        "            elif word_idx != previous_word_idx: # First token of a new word\n",
        "                # Assign the label of the current word\n",
        "                # Use .get() with default 'O' tag index for robustness\n",
        "                label_ids.append(tag2idx_map.get(label_list[word_idx], o_tag_idx))\n",
        "            else: # Subsequent tokens of the same word\n",
        "                # Assign -100 to ignore these subword tokens in loss calculation\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "print(\"Tokenizing and aligning labels for training set...\")\n",
        "train_encodings = tokenize_and_align_labels(train_sentences, train_tags, tokenizer, tag2idx)\n",
        "print(\"Tokenizing and aligning labels for test set...\")\n",
        "test_encodings = tokenize_and_align_labels(test_sentences, test_tags, tokenizer, tag2idx)\n",
        "\n",
        "# --- 4. Create PyTorch Dataset ---\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return tensors\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.labels)\n",
        "\n",
        "train_dataset = NERDataset(train_encodings)\n",
        "test_dataset = NERDataset(test_encodings) # Will split train_dataset later for validation\n",
        "\n",
        "print(f\"\\nSample Train Encoding (Input IDs): {train_dataset[0]['input_ids']}\")\n",
        "print(f\"Sample Train Encoding (Labels): {train_dataset[0]['labels']}\")\n",
        "\n",
        "# --- 5. Save Processed Data ---\n",
        "# Save datasets and tokenizer info for the training script\n",
        "processed_data = {\n",
        "    'train_dataset': train_dataset,\n",
        "    'test_dataset': test_dataset, # Note: This is the original test set\n",
        "    'tag2idx': tag2idx,\n",
        "    'idx2tag': idx2tag,\n",
        "    'n_tags': n_tags\n",
        "}\n",
        "with open(PREPARED_BERT_DATA_PATH, 'wb') as f:\n",
        "    pickle.dump(processed_data, f)\n",
        "print(f\"\\nProcessed data saved to {PREPARED_BERT_DATA_PATH}\")\n",
        "\n",
        "print(\"\\nBERT data preparation finished.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rl4LKRf2MJsV",
        "outputId": "bfa16284-833c-400c-fb9a-1064281ae419"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Loaded 2394 training sentences and 3850 test sentences.\n",
            "\n",
            "Creating tag mapping...\n",
            "Number of unique tags (from train set): 21\n",
            "Tag mapping: {'B-company': 0, 'B-facility': 1, 'B-geo-loc': 2, 'B-movie': 3, 'B-musicartist': 4, 'B-other': 5, 'B-person': 6, 'B-product': 7, 'B-sportsteam': 8, 'B-tvshow': 9, 'I-company': 10, 'I-facility': 11, 'I-geo-loc': 12, 'I-movie': 13, 'I-musicartist': 14, 'I-other': 15, 'I-person': 16, 'I-product': 17, 'I-sportsteam': 18, 'I-tvshow': 19, 'O': 20}\n",
            "Tag mapping saved to tag_map_bert.pkl\n",
            "\n",
            "Loading tokenizer: bert-base-uncased...\n",
            "Tokenizing and aligning labels for training set...\n",
            "Tokenizing and aligning labels for test set...\n",
            "\n",
            "Sample Train Encoding (Input IDs): tensor([  101,  1030,  3520,  9856, 27610, 25855,  2213,  1030,  1056,  2290,\n",
            "        10790,  2581,  2620,  2487,  2027,  2097,  2022,  2035,  2589,  2011,\n",
            "         4465,  3404,  2033,  1008, 16837,  1008,   102,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0])\n",
            "Sample Train Encoding (Labels): tensor([-100,   20, -100, -100, -100, -100, -100,   20, -100, -100, -100, -100,\n",
            "        -100, -100,   20,   20,   20,   20,   20,   20,   20,   20,   20,   20,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100])\n",
            "\n",
            "Processed data saved to prepared_bert_data.pkl\n",
            "\n",
            "BERT data preparation finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install seqeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlHx9yFdMyi2",
        "outputId": "7291450d-d603-42c0-e853-8bce5301f047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run BERT Training"
      ],
      "metadata": {
        "id": "VUSGovd7WGNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split, Dataset # Added Dataset import back\n",
        "from transformers import BertForTokenClassification, TrainingArguments, Trainer, BertTokenizerFast # Removed AdamW, Added BertTokenizerFast\n",
        "import numpy as np\n",
        "from seqeval.metrics import classification_report, f1_score, accuracy_score\n",
        "\n",
        "# --- Configuration ---\n",
        "PREPARED_BERT_DATA_PATH = \"prepared_bert_data.pkl\"\n",
        "MODEL_NAME = 'bert-base-uncased'\n",
        "OUTPUT_DIR = './bert_ner_output' # Directory to save model checkpoints and results\n",
        "LOGGING_DIR = './bert_ner_logs' # Directory for TensorBoard logs\n",
        "MODEL_SAVE_PATH = './bert_ner_final_model' # Directory to save the final fine-tuned model\n",
        "\n",
        "# Training Hyperparameters (can be tuned)\n",
        "LEARNING_RATE = 3e-5\n",
        "EPOCHS = 3 # Fewer epochs usually needed for fine-tuning BERT\n",
        "BATCH_SIZE = 16 # Adjust based on GPU memory\n",
        "WEIGHT_DECAY = 0.01\n",
        "VALIDATION_SPLIT_RATIO = 0.1 # Use 10% of training data for validation\n",
        "\n",
        "# --- Re-define NERDataset class (needed if loading from pickle) ---\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return tensors\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.labels)\n",
        "\n",
        "# --- 1. Load Processed Data ---\n",
        "print(\"Loading processed data...\")\n",
        "with open(PREPARED_BERT_DATA_PATH, 'rb') as f:\n",
        "    processed_data = pickle.load(f)\n",
        "\n",
        "# The datasets are already NERDataset objects from the previous script\n",
        "train_dataset_full = processed_data['train_dataset']\n",
        "test_dataset = processed_data['test_dataset']\n",
        "tag2idx = processed_data['tag2idx']\n",
        "idx2tag = processed_data['idx2tag']\n",
        "n_tags = processed_data['n_tags']\n",
        "\n",
        "print(\"Data loaded successfully.\")\n",
        "\n",
        "# --- 2. Split Training Data into Train/Validation ---\n",
        "print(\"Splitting training data into train/validation sets...\")\n",
        "train_size = int((1.0 - VALIDATION_SPLIT_RATIO) * len(train_dataset_full))\n",
        "val_size = len(train_dataset_full) - train_size\n",
        "train_dataset, val_dataset = random_split(train_dataset_full, [train_size, val_size])\n",
        "print(f\"Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}, Test size: {len(test_dataset)}\")\n",
        "\n",
        "# --- 3. Load Pre-trained Model ---\n",
        "print(f\"\\nLoading pre-trained model: {MODEL_NAME}...\")\n",
        "model = BertForTokenClassification.from_pretrained(MODEL_NAME, num_labels=n_tags)\n",
        "\n",
        "# --- 4. Define Metrics Computation ---\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (-100) and convert indices to labels\n",
        "    true_labels = []\n",
        "    true_predictions = []\n",
        "    for prediction_list, label_list in zip(predictions, labels):\n",
        "        temp_true = []\n",
        "        temp_pred = []\n",
        "        for prediction, label in zip(prediction_list, label_list):\n",
        "            if label != -100: # Only consider non-ignored labels\n",
        "                temp_true.append(idx2tag[label])\n",
        "                temp_pred.append(idx2tag[prediction])\n",
        "        true_labels.append(temp_true)\n",
        "        true_predictions.append(temp_pred)\n",
        "\n",
        "    # Use seqeval to compute metrics\n",
        "    report = classification_report(true_labels, true_predictions, output_dict=True, zero_division=0)\n",
        "\n",
        "    # Extract overall metrics (micro avg is common for NER)\n",
        "    results = {\n",
        "        \"precision\": report[\"micro avg\"][\"precision\"],\n",
        "        \"recall\": report[\"micro avg\"][\"recall\"],\n",
        "        \"f1\": report[\"micro avg\"][\"f1-score\"],\n",
        "        \"accuracy\": accuracy_score(true_labels, true_predictions),\n",
        "    }\n",
        "    return results\n",
        "\n",
        "# --- 5. Define Training Arguments ---\n",
        "print(\"\\nSetting up training arguments...\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    logging_dir=LOGGING_DIR,\n",
        "    logging_steps=50, # Log metrics less frequently\n",
        "    evaluation_strategy=\"epoch\", # Evaluate at the end of each epoch\n",
        "    save_strategy=\"epoch\", # Save model checkpoint at the end of each epoch\n",
        "    load_best_model_at_end=True, # Load the best model based on validation loss at the end\n",
        "    metric_for_best_model=\"eval_loss\", # Use validation loss to determine the best model\n",
        "    greater_is_better=False, # Lower validation loss is better\n",
        "    report_to=\"tensorboard\" # Log to TensorBoard\n",
        ")\n",
        "\n",
        "# --- 6. Initialize Trainer ---\n",
        "print(\"Initializing Trainer...\")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# --- 7. Train the Model ---\n",
        "print(\"\\nStarting training...\")\n",
        "trainer.train()\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# --- 8. Evaluate on Test Set ---\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
        "print(\"\\nTest Set Evaluation Results:\")\n",
        "print(test_results)\n",
        "\n",
        "# --- 9. Save Final Model and Tokenizer ---\n",
        "print(f\"\\nSaving final model to {MODEL_SAVE_PATH}...\")\n",
        "trainer.save_model(MODEL_SAVE_PATH)\n",
        "# Tokenizer was loaded in the previous script, saving it here too for completeness\n",
        "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME) # Re-load tokenizer\n",
        "tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
        "print(\"Model and tokenizer saved successfully.\")\n",
        "\n",
        "print(\"\\nBERT fine-tuning and evaluation complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "tBB_NwsjMe2l",
        "outputId": "5a0b907d-d309-4537-e613-7b5b546e5308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading processed data...\n",
            "Data loaded successfully.\n",
            "Splitting training data into train/validation sets...\n",
            "Train size: 2154, Validation size: 240, Test size: 3850\n",
            "\n",
            "Loading pre-trained model: bert-base-uncased...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Setting up training arguments...\n",
            "Initializing Trainer...\n",
            "\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='405' max='405' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [405/405 02:40, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.260100</td>\n",
              "      <td>0.256610</td>\n",
              "      <td>0.278195</td>\n",
              "      <td>0.217647</td>\n",
              "      <td>0.244224</td>\n",
              "      <td>0.946957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.155200</td>\n",
              "      <td>0.208706</td>\n",
              "      <td>0.553957</td>\n",
              "      <td>0.452941</td>\n",
              "      <td>0.498382</td>\n",
              "      <td>0.958580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.108700</td>\n",
              "      <td>0.213389</td>\n",
              "      <td>0.491525</td>\n",
              "      <td>0.511765</td>\n",
              "      <td>0.501441</td>\n",
              "      <td>0.957523</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training finished.\n",
            "\n",
            "Evaluating on test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='241' max='241' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [241/241 00:26]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Set Evaluation Results:\n",
            "{'eval_loss': 0.34703338146209717, 'eval_precision': 0.30686695278969955, 'eval_recall': 0.2470486610999136, 'eval_f1': 0.27372786728345827, 'eval_accuracy': 0.9197182871080815, 'eval_runtime': 27.5699, 'eval_samples_per_second': 139.645, 'eval_steps_per_second': 8.741, 'epoch': 3.0}\n",
            "\n",
            "Saving final model to ./bert_ner_final_model...\n",
            "Model and tokenizer saved successfully.\n",
            "\n",
            "BERT fine-tuning and evaluation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict BERT"
      ],
      "metadata": {
        "id": "yWLcgh13WV7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizerFast, BertForTokenClassification\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_PATH = './bert_ner_final_model' # Path where the fine-tuned model and tokenizer are saved\n",
        "TAG_MAP_PATH = \"tag_map_bert.pkl\"\n",
        "\n",
        "# --- Load Model, Tokenizer, and Tag Mapping ---\n",
        "print(f\"Loading model and tokenizer from {MODEL_PATH}...\")\n",
        "model = BertForTokenClassification.from_pretrained(MODEL_PATH)\n",
        "tokenizer = BertTokenizerFast.from_pretrained(MODEL_PATH)\n",
        "\n",
        "print(f\"Loading tag mapping from {TAG_MAP_PATH}...\")\n",
        "with open(TAG_MAP_PATH, 'rb') as f:\n",
        "    tag_maps = pickle.load(f)\n",
        "idx2tag = tag_maps['idx2tag']\n",
        "tag2idx = tag_maps['tag2idx'] # Needed for potential checks, though idx2tag is primary for output\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Prediction Function ---\n",
        "def predict_ner(sentence, model, tokenizer, idx2tag_map):\n",
        "    \"\"\"Predicts NER tags for a given sentence.\"\"\"\n",
        "    # Tokenize the sentence\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "    # Move inputs to the correct device\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Get model predictions\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # Get the most likely tag index for each token\n",
        "    predictions = torch.argmax(logits, dim=2)\n",
        "\n",
        "    # Convert token IDs and prediction indices back to words and tags\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].cpu().numpy())\n",
        "    predicted_indices = predictions[0].cpu().numpy()\n",
        "\n",
        "    # Align tokens and predictions (ignoring special tokens and padding)\n",
        "    word_tags = []\n",
        "    current_word = \"\"\n",
        "    current_tag_idx = -1 # Initialize with an invalid index\n",
        "\n",
        "    # Use word_ids to group subword tokens\n",
        "    word_ids = inputs.word_ids(batch_index=0)\n",
        "    previous_word_idx = None\n",
        "\n",
        "    for i, token in enumerate(tokens):\n",
        "        if token in [tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token]:\n",
        "            continue\n",
        "\n",
        "        word_idx = word_ids[i]\n",
        "        tag_idx = predicted_indices[i]\n",
        "\n",
        "        if word_idx is None: # Should not happen if we skip special tokens, but good check\n",
        "             continue\n",
        "\n",
        "        # If it's the start of a new word (or the first word)\n",
        "        if word_idx != previous_word_idx:\n",
        "            # Add the previous word and its tag if it exists\n",
        "            if current_word:\n",
        "                 word_tags.append((current_word, idx2tag_map.get(current_tag_idx, 'O'))) # Default to 'O'\n",
        "\n",
        "            # Start the new word\n",
        "            current_word = token\n",
        "            current_tag_idx = tag_idx\n",
        "        else: # It's a subword token, append to the current word\n",
        "            # Remove '##' prefix if present\n",
        "            current_word += token.replace('##', '')\n",
        "            # Keep the tag of the first subword token (common strategy)\n",
        "            # current_tag_idx remains unchanged\n",
        "\n",
        "        previous_word_idx = word_idx\n",
        "\n",
        "    # Add the last word\n",
        "    if current_word:\n",
        "         word_tags.append((current_word, idx2tag_map.get(current_tag_idx, 'O')))\n",
        "\n",
        "    return word_tags\n",
        "\n",
        "# --- Example Sentences ---\n",
        "sentences_to_predict = [\n",
        "    \"Harry Potter went to London to watch the Arsenal game.\",\n",
        "    \"Apple announced the new iPhone at the Steve Jobs Theater in Cupertino.\",\n",
        "    \"Taylor Swift released her album 'Folklore' last year.\",\n",
        "    \"Watching The Office on Netflix is my favorite pastime.\"\n",
        "]\n",
        "\n",
        "# --- Perform Predictions ---\n",
        "print(\"\\nPerforming predictions on custom sentences:\")\n",
        "for sentence in sentences_to_predict:\n",
        "    print(f\"\\nSentence: {sentence}\")\n",
        "    predicted_tags = predict_ner(sentence, model, tokenizer, idx2tag)\n",
        "    print(\"Predicted Tags:\", predicted_tags)\n",
        "\n",
        "print(\"\\nPrediction complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlLBuO4MMsxW",
        "outputId": "4ee7e883-df01-44d0-96e9-dab93139af46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model and tokenizer from ./bert_ner_final_model...\n",
            "Loading tag mapping from tag_map_bert.pkl...\n",
            "Using device: cuda\n",
            "\n",
            "Performing predictions on custom sentences:\n",
            "\n",
            "Sentence: Harry Potter went to London to watch the Arsenal game.\n",
            "Predicted Tags: [('harry', 'B-person'), ('potter', 'I-person'), ('went', 'O'), ('to', 'O'), ('london', 'B-geo-loc'), ('to', 'O'), ('watch', 'O'), ('the', 'O'), ('arsenal', 'B-person'), ('game', 'O'), ('.', 'O')]\n",
            "\n",
            "Sentence: Apple announced the new iPhone at the Steve Jobs Theater in Cupertino.\n",
            "Predicted Tags: [('apple', 'B-company'), ('announced', 'O'), ('the', 'O'), ('new', 'O'), ('iphone', 'O'), ('at', 'O'), ('the', 'O'), ('steve', 'B-person'), ('jobs', 'O'), ('theater', 'O'), ('in', 'O'), ('cupertino', 'O'), ('.', 'O')]\n",
            "\n",
            "Sentence: Taylor Swift released her album 'Folklore' last year.\n",
            "Predicted Tags: [('taylor', 'B-person'), ('swift', 'I-person'), ('released', 'O'), ('her', 'O'), ('album', 'O'), (\"'\", 'O'), ('folklore', 'O'), (\"'\", 'O'), ('last', 'O'), ('year', 'O'), ('.', 'O')]\n",
            "\n",
            "Sentence: Watching The Office on Netflix is my favorite pastime.\n",
            "Predicted Tags: [('watching', 'O'), ('the', 'O'), ('office', 'O'), ('on', 'O'), ('netflix', 'O'), ('is', 'O'), ('my', 'O'), ('favorite', 'O'), ('pastime', 'O'), ('.', 'O')]\n",
            "\n",
            "Prediction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train LSTM CRF"
      ],
      "metadata": {
        "id": "0FV40vGuZl5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle # To save tokenizer and mappings\n",
        "\n",
        "# --- Data Loading Function (copied from preprocess_data.py) ---\n",
        "def load_conll_data(file_path):\n",
        "    \"\"\"Loads data from a CoNLL formatted file.\"\"\"\n",
        "    sentences = []\n",
        "    tags = []\n",
        "    current_sentence = []\n",
        "    current_tags = []\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "        return [], []\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line == \"\":\n",
        "                    if current_sentence:\n",
        "                        sentences.append(current_sentence)\n",
        "                        tags.append(current_tags)\n",
        "                        current_sentence = []\n",
        "                        current_tags = []\n",
        "                else:\n",
        "                    parts = line.split()\n",
        "                    if len(parts) >= 2:\n",
        "                        word = parts[0]\n",
        "                        tag = parts[-1]\n",
        "                        current_sentence.append(word)\n",
        "                        current_tags.append(tag)\n",
        "                    else:\n",
        "                        print(f\"Skipping malformed line: '{line}' in file {file_path}\")\n",
        "            if current_sentence:\n",
        "                sentences.append(current_sentence)\n",
        "                tags.append(current_tags)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file {file_path}: {e}\")\n",
        "        return [], []\n",
        "    return sentences, tags\n",
        "# --- End of Data Loading Function ---\n",
        "\n",
        "# --- Configuration ---\n",
        "TRAIN_FILE = \"wnut 16.txt.conll\"\n",
        "TEST_FILE = \"wnut 16test.txt.conll\"\n",
        "W2V_MODEL_PATH = \"word2vec_lstm.model\"\n",
        "TOKENIZER_PATH = \"tokenizer_lstm.pkl\"\n",
        "TAG_MAP_PATH = \"tag_map_lstm.pkl\"\n",
        "EMBEDDING_MATRIX_PATH = \"embedding_matrix_lstm.npy\" # Added path for saving matrix\n",
        "PREPARED_DATA_PATH = \"prepared_lstm_data.pkl\" # Added path for saving prepared data\n",
        "\n",
        "EMBEDDING_DIM = 100  # Dimension for Word2Vec and Embedding layer\n",
        "MAX_SEQ_LEN = 50     # Maximum sequence length after padding\n",
        "VALIDATION_SPLIT = 0.2\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "print(\"Loading data...\")\n",
        "train_sentences, train_tags = load_conll_data(TRAIN_FILE)\n",
        "test_sentences, test_tags = load_conll_data(TEST_FILE)\n",
        "\n",
        "if not train_sentences or not test_sentences:\n",
        "    print(\"Failed to load data. Exiting.\")\n",
        "    exit()\n",
        "\n",
        "print(f\"Loaded {len(train_sentences)} training sentences and {len(test_sentences)} test sentences.\")\n",
        "\n",
        "# --- 2. Train Word2Vec ---\n",
        "print(\"\\nTraining Word2Vec model...\")\n",
        "# Combine train and test sentences for a richer vocabulary\n",
        "all_sentences = train_sentences + test_sentences\n",
        "w2v_model = Word2Vec(sentences=all_sentences, vector_size=EMBEDDING_DIM, window=5, min_count=1, workers=4, sg=1) # Using Skip-gram\n",
        "w2v_model.save(W2V_MODEL_PATH)\n",
        "print(f\"Word2Vec model saved to {W2V_MODEL_PATH}\")\n",
        "print(f\"Vocabulary size: {len(w2v_model.wv.index_to_key)}\")\n",
        "\n",
        "# --- 3. Prepare Data for TensorFlow ---\n",
        "print(\"\\nPreparing data for TensorFlow...\")\n",
        "\n",
        "# 3.1. Create Tag Mapping\n",
        "all_tags_flat = [tag for sublist in train_tags + test_tags for tag in sublist]\n",
        "unique_tags = sorted(list(set(all_tags_flat)))\n",
        "tag2idx = {tag: i for i, tag in enumerate(unique_tags)}\n",
        "idx2tag = {i: tag for tag, i in tag2idx.items()}\n",
        "n_tags = len(unique_tags)\n",
        "print(f\"Number of unique tags: {n_tags}\")\n",
        "print(f\"Tag mapping: {tag2idx}\")\n",
        "\n",
        "# Save tag mapping\n",
        "with open(TAG_MAP_PATH, 'wb') as f:\n",
        "    pickle.dump({'tag2idx': tag2idx, 'idx2tag': idx2tag}, f)\n",
        "print(f\"Tag mapping saved to {TAG_MAP_PATH}\")\n",
        "\n",
        "# 3.2. Tokenize Words\n",
        "# Use Keras Tokenizer, fit on training sentences only to avoid data leakage\n",
        "word_tokenizer = Tokenizer(oov_token=\"<OOV>\") # Out-of-vocabulary token\n",
        "word_tokenizer.fit_on_texts(train_sentences)\n",
        "vocab_size = len(word_tokenizer.word_index) + 1 # +1 for padding token 0\n",
        "print(f\"Vocabulary size (Keras Tokenizer): {vocab_size}\")\n",
        "\n",
        "# Save tokenizer\n",
        "with open(TOKENIZER_PATH, 'wb') as f:\n",
        "    pickle.dump(word_tokenizer, f)\n",
        "print(f\"Tokenizer saved to {TOKENIZER_PATH}\")\n",
        "\n",
        "# 3.3. Convert Sentences and Tags to Sequences\n",
        "X_train = word_tokenizer.texts_to_sequences(train_sentences)\n",
        "y_train = [[tag2idx[tag] for tag in tags] for tags in train_tags]\n",
        "\n",
        "X_test = word_tokenizer.texts_to_sequences(test_sentences)\n",
        "y_test = [[tag2idx[tag] for tag in tags] for tags in test_tags]\n",
        "\n",
        "# 3.4. Pad Sequences\n",
        "print(f\"\\nPadding sequences to max length: {MAX_SEQ_LEN}...\")\n",
        "X_train_padded = pad_sequences(X_train, maxlen=MAX_SEQ_LEN, padding='post')\n",
        "y_train_padded = pad_sequences(y_train, maxlen=MAX_SEQ_LEN, padding='post', value=tag2idx['O']) # Pad tags with 'O' tag index\n",
        "\n",
        "X_test_padded = pad_sequences(X_test, maxlen=MAX_SEQ_LEN, padding='post')\n",
        "y_test_padded = pad_sequences(y_test, maxlen=MAX_SEQ_LEN, padding='post', value=tag2idx['O'])\n",
        "\n",
        "print(\"Padding complete.\")\n",
        "\n",
        "# --- 4. Train/Validation Split ---\n",
        "print(\"\\nSplitting data into training and validation sets...\")\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "    X_train_padded, y_train_padded, test_size=VALIDATION_SPLIT, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "print(f\"Training sequences shape: {X_train_split.shape}\")\n",
        "print(f\"Training tags shape: {y_train_split.shape}\")\n",
        "print(f\"Validation sequences shape: {X_val_split.shape}\")\n",
        "print(f\"Validation tags shape: {y_val_split.shape}\")\n",
        "print(f\"Test sequences shape: {X_test_padded.shape}\")\n",
        "print(f\"Test tags shape: {y_test_padded.shape}\")\n",
        "\n",
        "# --- 5. Create Embedding Matrix (using Word2Vec) ---\n",
        "print(\"\\nCreating embedding matrix...\")\n",
        "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
        "hits = 0\n",
        "misses = 0\n",
        "for word, i in word_tokenizer.word_index.items():\n",
        "    if word in w2v_model.wv:\n",
        "        embedding_matrix[i] = w2v_model.wv[word]\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "        # Words not found in Word2Vec will be initialized to zero vectors.\n",
        "\n",
        "print(f\"Converted {hits} words ({misses} misses)\")\n",
        "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
        "\n",
        "# Save embedding matrix\n",
        "np.save(EMBEDDING_MATRIX_PATH, embedding_matrix)\n",
        "print(f\"Embedding matrix saved to {EMBEDDING_MATRIX_PATH}\")\n",
        "\n",
        "# Save prepared data splits for later use in model training script\n",
        "prepared_data = {\n",
        "    'X_train': X_train_split,\n",
        "    'y_train': y_train_split,\n",
        "    'X_val': X_val_split,\n",
        "    'y_val': y_val_split,\n",
        "    'X_test': X_test_padded,\n",
        "    'y_test': y_test_padded,\n",
        "    'vocab_size': vocab_size,\n",
        "    'n_tags': n_tags,\n",
        "    'max_seq_len': MAX_SEQ_LEN\n",
        "}\n",
        "with open(PREPARED_DATA_PATH, 'wb') as f:\n",
        "    pickle.dump(prepared_data, f)\n",
        "print(f\"Prepared data saved to {PREPARED_DATA_PATH}\")\n",
        "\n",
        "\n",
        "print(\"\\nData preparation finished. Ready for model building and training.\")\n"
      ],
      "metadata": {
        "id": "Y426mEGbZiuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run LSTM CRF Training"
      ],
      "metadata": {
        "id": "m46-qqMtZwQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, TimeDistributed, Dense, Activation\n",
        "# Note: CRF layer attempts using tensorflow-addons and keras-crf failed due to library incompatibilities/deprecation.\n",
        "# This script now trains a standard BiLSTM model without a CRF layer.\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from seqeval.metrics import classification_report, f1_score, accuracy_score # Added accuracy_score back for consistency\n",
        "\n",
        "# --- Configuration ---\n",
        "PREPARED_DATA_PATH = \"prepared_lstm_data.pkl\"\n",
        "EMBEDDING_MATRIX_PATH = \"embedding_matrix_lstm.npy\"\n",
        "TAG_MAP_PATH = \"tag_map_lstm.pkl\"\n",
        "MODEL_SAVE_PATH = \"bilstm_model.h5\" # Changed to .h5 format\n",
        "\n",
        "# Hyperparameters (can be tuned later)\n",
        "LSTM_UNITS = 64\n",
        "DROPOUT_RATE = 0.1 # Added dropout for regularization\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 15 # Increased epochs, will use EarlyStopping\n",
        "BATCH_SIZE = 32\n",
        "PATIENCE = 3 # For EarlyStopping\n",
        "\n",
        "# --- 1. Load Prepared Data and Artifacts ---\n",
        "print(\"Loading prepared data and artifacts...\")\n",
        "with open(PREPARED_DATA_PATH, 'rb') as f:\n",
        "    prepared_data = pickle.load(f)\n",
        "\n",
        "X_train = prepared_data['X_train']\n",
        "y_train = prepared_data['y_train']\n",
        "X_val = prepared_data['X_val']\n",
        "y_val = prepared_data['y_val']\n",
        "X_test = prepared_data['X_test']\n",
        "y_test = prepared_data['y_test']\n",
        "vocab_size = prepared_data['vocab_size']\n",
        "n_tags = prepared_data['n_tags']\n",
        "max_seq_len = prepared_data['max_seq_len']\n",
        "\n",
        "embedding_matrix = np.load(EMBEDDING_MATRIX_PATH)\n",
        "embedding_dim = embedding_matrix.shape[1] # Get embedding dim from matrix\n",
        "\n",
        "with open(TAG_MAP_PATH, 'rb') as f:\n",
        "    tag_maps = pickle.load(f)\n",
        "idx2tag = tag_maps['idx2tag']\n",
        "\n",
        "print(\"Data loaded successfully.\")\n",
        "print(f\"Vocab size: {vocab_size}, Embedding dim: {embedding_dim}, Max seq len: {max_seq_len}, Num tags: {n_tags}\")\n",
        "\n",
        "# --- 2. Build BiLSTM Model ---\n",
        "print(\"\\nBuilding BiLSTM model (CRF attempts failed due to library issues)...\")\n",
        "\n",
        "# Input layer\n",
        "input_layer = Input(shape=(max_seq_len,))\n",
        "\n",
        "# Embedding layer (using pre-trained Word2Vec weights)\n",
        "# Set trainable=False initially, can be fine-tuned later if needed\n",
        "embedding_layer = Embedding(input_dim=vocab_size,\n",
        "                            output_dim=embedding_dim,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=max_seq_len,\n",
        "                            mask_zero=True, # Important for handling padding\n",
        "                            trainable=True)(input_layer) # Allow fine-tuning embeddings\n",
        "\n",
        "# Bidirectional LSTM layer\n",
        "# return_sequences=True is crucial for sequence labeling\n",
        "bilstm_layer = Bidirectional(LSTM(units=LSTM_UNITS, return_sequences=True, dropout=DROPOUT_RATE, recurrent_dropout=DROPOUT_RATE))(embedding_layer)\n",
        "\n",
        "# TimeDistributed Dense layer\n",
        "# TimeDistributed Dense layer with softmax activation for standard sequence classification\n",
        "time_distributed_dense = TimeDistributed(Dense(n_tags))(bilstm_layer)\n",
        "output_layer = Activation('softmax')(time_distributed_dense)\n",
        "\n",
        "# Define the model\n",
        "model = Model(input_layer, output_layer)\n",
        "model.summary()\n",
        "\n",
        "# --- 3. Compile Model ---\n",
        "print(\"\\nCompiling model...\")\n",
        "# Use the CRF potential function as the loss\n",
        "# The CRF layer handles the decoding and loss calculation internally when used as the output layer.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "# Compile with sparse_categorical_crossentropy loss and accuracy metric.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# --- 4. Train Model ---\n",
        "print(\"\\nTraining model...\")\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint(MODEL_SAVE_PATH, monitor='val_loss', save_best_only=True)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stopping, model_checkpoint]\n",
        ")\n",
        "\n",
        "print(\"Training finished.\")\n",
        "print(f\"Best model saved to {MODEL_SAVE_PATH}\")\n",
        "\n",
        "# --- 5. Evaluate Model ---\n",
        "print(\"\\nEvaluating model on test set...\")\n",
        "\n",
        "# Predict probabilities for each tag at each time step\n",
        "y_pred_probs = model.predict(X_test)\n",
        "# Get the tag index with the highest probability at each step\n",
        "y_pred_indices = np.argmax(y_pred_probs, axis=-1)\n",
        "\n",
        "# Convert indices back to tags, ignoring padding (where X_test is 0)\n",
        "y_true_tags = []\n",
        "y_pred_tags = []\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "    true_seq = []\n",
        "    pred_seq = []\n",
        "    for j in range(max_seq_len):\n",
        "        if X_test[i, j] != 0: # Check if it's not a padding token\n",
        "            true_tag_idx = y_test[i, j]\n",
        "            pred_tag_idx = y_pred_indices[i, j]\n",
        "            true_seq.append(idx2tag[true_tag_idx])\n",
        "            pred_seq.append(idx2tag[pred_tag_idx])\n",
        "    y_true_tags.append(true_seq)\n",
        "    y_pred_tags.append(pred_seq)\n",
        "\n",
        "# Calculate and print classification report\n",
        "report = classification_report(y_true_tags, y_pred_tags, digits=4)\n",
        "print(\"\\nClassification Report (Test Set):\")\n",
        "print(report)\n",
        "\n",
        "# Calculate overall F1 score (micro average is often used for NER)\n",
        "f1 = f1_score(y_true_tags, y_pred_tags, average='micro')\n",
        "print(f\"\\nOverall F1 Score (Micro): {f1:.4f}\")\n",
        "\n",
        "print(\"\\nEvaluation complete.\")\n"
      ],
      "metadata": {
        "id": "fJxm_C-IZsQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict LSTM"
      ],
      "metadata": {
        "id": "NvY1IHxIZ7E5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_PATH = \"bilstm_model.h5\"\n",
        "TOKENIZER_PATH = \"tokenizer_lstm.pkl\"\n",
        "TAG_MAP_PATH = \"tag_map_lstm.pkl\"\n",
        "# Load MAX_SEQ_LEN from the prepared data (or define it if known)\n",
        "# We'll load it from the prepared data pickle for consistency\n",
        "PREPARED_DATA_PATH = \"prepared_lstm_data.pkl\"\n",
        "\n",
        "# --- Load Model, Tokenizer, Tag Map, and Config ---\n",
        "print(f\"Loading model from {MODEL_PATH}...\")\n",
        "# No custom objects needed as we reverted to standard layers\n",
        "model = tf.keras.models.load_model(MODEL_PATH)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "print(f\"Loading tokenizer from {TOKENIZER_PATH}...\")\n",
        "with open(TOKENIZER_PATH, 'rb') as f:\n",
        "    word_tokenizer = pickle.load(f)\n",
        "print(\"Tokenizer loaded successfully.\")\n",
        "\n",
        "print(f\"Loading tag mapping from {TAG_MAP_PATH}...\")\n",
        "with open(TAG_MAP_PATH, 'rb') as f:\n",
        "    tag_maps = pickle.load(f)\n",
        "idx2tag = tag_maps['idx2tag']\n",
        "print(\"Tag mapping loaded successfully.\")\n",
        "\n",
        "print(f\"Loading max sequence length from {PREPARED_DATA_PATH}...\")\n",
        "try:\n",
        "    with open(PREPARED_DATA_PATH, 'rb') as f:\n",
        "        prepared_data = pickle.load(f)\n",
        "    MAX_SEQ_LEN = prepared_data['max_seq_len']\n",
        "    print(f\"Max sequence length set to: {MAX_SEQ_LEN}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: {PREPARED_DATA_PATH} not found. Using default MAX_SEQ_LEN=50.\")\n",
        "    MAX_SEQ_LEN = 50 # Fallback if the prepared data file is missing\n",
        "except KeyError:\n",
        "    print(f\"Error: 'max_seq_len' key not found in {PREPARED_DATA_PATH}. Using default MAX_SEQ_LEN=50.\")\n",
        "    MAX_SEQ_LEN = 50 # Fallback if the key is missing\n",
        "\n",
        "# --- Prediction Function ---\n",
        "def predict_ner_lstm(sentence, model, tokenizer, idx2tag_map, max_len):\n",
        "    \"\"\"Predicts NER tags for a given sentence using the BiLSTM model.\"\"\"\n",
        "    # Tokenize the sentence words\n",
        "    words = sentence.split() # Simple split for this example\n",
        "    word_sequences = tokenizer.texts_to_sequences([words])\n",
        "\n",
        "    # Pad the sequence\n",
        "    padded_sequence = pad_sequences(word_sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "    # Get model predictions (probabilities)\n",
        "    pred_probs = model.predict(padded_sequence, verbose=0) # verbose=0 to suppress progress bar\n",
        "\n",
        "    # Get the tag index with the highest probability for each token\n",
        "    pred_indices = np.argmax(pred_probs, axis=-1)[0] # Get the predictions for the first (only) sequence\n",
        "\n",
        "    # Convert indices back to tags\n",
        "    predicted_tags = [idx2tag_map.get(idx, 'O') for idx in pred_indices[:len(words)]] # Only map tags for original words\n",
        "\n",
        "    # Combine words and predicted tags\n",
        "    word_tags = list(zip(words, predicted_tags))\n",
        "\n",
        "    return word_tags\n",
        "\n",
        "# --- Example Sentences ---\n",
        "sentences_to_predict = [\n",
        "    \"Harry Potter went to London to watch the Arsenal game.\",\n",
        "    \"Apple announced the new iPhone at the Steve Jobs Theater in Cupertino.\",\n",
        "    \"Taylor Swift released her album 'Folklore' last year.\",\n",
        "    \"Watching The Office on Netflix is my favorite pastime.\"\n",
        "]\n",
        "\n",
        "# --- Perform Predictions ---\n",
        "print(\"\\nPerforming predictions on custom sentences (BiLSTM Model):\")\n",
        "for sentence in sentences_to_predict:\n",
        "    print(f\"\\nSentence: {sentence}\")\n",
        "    predicted_tags = predict_ner_lstm(sentence, model, word_tokenizer, idx2tag, MAX_SEQ_LEN)\n",
        "    print(\"Predicted Tags:\", predicted_tags)\n",
        "\n",
        "print(\"\\nPrediction complete.\")\n"
      ],
      "metadata": {
        "id": "G2XnS4wvZ2ru"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}